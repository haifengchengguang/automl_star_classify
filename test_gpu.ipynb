{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"test_gpu\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"test_gpu\\\"\n",
      "AutoGluon Version:  0.4.0\n",
      "Python Version:     3.9.11\n",
      "Operating System:   Windows\n",
      "Train Data Rows:    39073\n",
      "Train Data Columns: 14\n",
      "Label Column: class\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [' <=50K', ' >50K']\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 =  >50K, class 0 =  <=50K\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive ( >50K) vs negative ( <=50K) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4444.42 MB\n",
      "\tTrain Data (Original)  Memory Usage: 22.92 MB (0.5% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', [])    : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
      "\t\t('object', []) : 8 | ['workclass', 'education', 'marital-status', 'occupation', 'relationship', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 7 | ['workclass', 'education', 'marital-status', 'occupation', 'relationship', ...]\n",
      "\t\t('int', [])       : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
      "\t\t('int', ['bool']) : 1 | ['sex']\n",
      "\t0.3s = Fit runtime\n",
      "\t14 features in original data used to generate 14 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 2.19 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.35s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.0639828014229775, Train Rows: 36573, Val Rows: 2500\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.7752\t = Validation score   (accuracy)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.766\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t0.8792\t = Validation score   (accuracy)\n",
      "\t22.12s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t0.8824\t = Validation score   (accuracy)\n",
      "\t5.97s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.864\t = Validation score   (accuracy)\n",
      "\t2.35s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.8608\t = Validation score   (accuracy)\n",
      "\t2.69s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t0.8816\t = Validation score   (accuracy)\n",
      "\t14.69s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.8496\t = Validation score   (accuracy)\n",
      "\t1.64s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.8496\t = Validation score   (accuracy)\n",
      "\t1.68s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training... Skipping this model.\n",
      "\t\t\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1074, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, **model_fit_kwargs)\n",
      "  File \"D:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1032, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, **model_fit_kwargs)\n",
      "  File \"D:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 577, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"D:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\tabular_nn_fastai.py\", line 200, in _fit\n",
      "    torch_core.default_device(use_cuda=True)\n",
      "  File \"D:\\conda\\envs\\automl\\lib\\site-packages\\fastai\\torch_core.py\", line 247, in default_device\n",
      "    assert torch.cuda.is_available() or not use\n",
      "AssertionError\n",
      "Fitting model: XGBoost ...\n",
      "D:\\conda\\envs\\automl\\lib\\site-packages\\xgboost\\compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "\t0.8848\t = Validation score   (accuracy)\n",
      "\t3.18s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      3\u001B[0m test_data \u001B[38;5;241m=\u001B[39m TabularDataset(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://autogluon.s3.amazonaws.com/datasets/Inc/test.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      4\u001B[0m savepath\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest_gpu\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m----> 5\u001B[0m predictor \u001B[38;5;241m=\u001B[39m \u001B[43mTabularPredictor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mclass\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msavepath\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43mag_args_fit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnum_gpus\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m predictions \u001B[38;5;241m=\u001B[39m predictor\u001B[38;5;241m.\u001B[39mpredict(test_data)\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\core\\utils\\decorators.py:30\u001B[0m, in \u001B[0;36munpack.<locals>._unpack_inner.<locals>._call\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(f)\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_call\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     29\u001B[0m     gargs, gkwargs \u001B[38;5;241m=\u001B[39m g(\u001B[38;5;241m*\u001B[39mother_args, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m---> 30\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39mgargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mgkwargs)\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:813\u001B[0m, in \u001B[0;36mTabularPredictor.fit\u001B[1;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, **kwargs)\u001B[0m\n\u001B[0;32m    805\u001B[0m core_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    806\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mag_args\u001B[39m\u001B[38;5;124m'\u001B[39m: ag_args,\n\u001B[0;32m    807\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mag_args_ensemble\u001B[39m\u001B[38;5;124m'\u001B[39m: ag_args_ensemble,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    810\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfeature_prune_kwargs\u001B[39m\u001B[38;5;124m'\u001B[39m: kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfeature_prune_kwargs\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    811\u001B[0m }\n\u001B[0;32m    812\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave(silent\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)  \u001B[38;5;66;03m# Save predictor to disk to enable prediction and training after interrupt\u001B[39;00m\n\u001B[1;32m--> 813\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_learner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_val\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtuning_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_unlabeled\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43munlabeled_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    814\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mholdout_frac\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mholdout_frac\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_bag_folds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_bag_folds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_bag_sets\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_bag_sets\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    815\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mnum_stack_levels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_stack_levels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    816\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mhyperparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhyperparameters\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcore_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcore_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    817\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mtime_limit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtime_limit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minfer_limit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_limit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minfer_limit_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_limit_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    818\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mverbosity\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbosity\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_bag_holdout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_bag_holdout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    819\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_post_fit_vars()\n\u001B[0;32m    821\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_post_fit(\n\u001B[0;32m    822\u001B[0m     keep_only_best\u001B[38;5;241m=\u001B[39mkwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mkeep_only_best\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    823\u001B[0m     refit_full\u001B[38;5;241m=\u001B[39mkwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrefit_full\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    827\u001B[0m     infer_limit\u001B[38;5;241m=\u001B[39minfer_limit,\n\u001B[0;32m    828\u001B[0m )\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\tabular\\learner\\abstract_learner.py:150\u001B[0m, in \u001B[0;36mAbstractLearner.fit\u001B[1;34m(self, X, X_val, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLearner is already fit.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_fit_input(X\u001B[38;5;241m=\u001B[39mX, X_val\u001B[38;5;241m=\u001B[39mX_val, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m--> 150\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(X\u001B[38;5;241m=\u001B[39mX, X_val\u001B[38;5;241m=\u001B[39mX_val, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\tabular\\learner\\default_learner.py:124\u001B[0m, in \u001B[0;36mDefaultLearner._fit\u001B[1;34m(self, X, X_val, X_unlabeled, holdout_frac, num_bag_folds, num_bag_sets, time_limit, infer_limit, infer_limit_batch_size, verbosity, **trainer_fit_kwargs)\u001B[0m\n\u001B[0;32m    121\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39meval_metric \u001B[38;5;241m=\u001B[39m trainer\u001B[38;5;241m.\u001B[39meval_metric\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave()\n\u001B[1;32m--> 124\u001B[0m trainer\u001B[38;5;241m.\u001B[39mfit(\n\u001B[0;32m    125\u001B[0m     X\u001B[38;5;241m=\u001B[39mX,\n\u001B[0;32m    126\u001B[0m     y\u001B[38;5;241m=\u001B[39my,\n\u001B[0;32m    127\u001B[0m     X_val\u001B[38;5;241m=\u001B[39mX_val,\n\u001B[0;32m    128\u001B[0m     y_val\u001B[38;5;241m=\u001B[39my_val,\n\u001B[0;32m    129\u001B[0m     X_unlabeled\u001B[38;5;241m=\u001B[39mX_unlabeled,\n\u001B[0;32m    130\u001B[0m     holdout_frac\u001B[38;5;241m=\u001B[39mholdout_frac,\n\u001B[0;32m    131\u001B[0m     time_limit\u001B[38;5;241m=\u001B[39mtime_limit_trainer,\n\u001B[0;32m    132\u001B[0m     infer_limit\u001B[38;5;241m=\u001B[39minfer_limit,\n\u001B[0;32m    133\u001B[0m     infer_limit_batch_size\u001B[38;5;241m=\u001B[39minfer_limit_batch_size,\n\u001B[0;32m    134\u001B[0m     groups\u001B[38;5;241m=\u001B[39mgroups,\n\u001B[0;32m    135\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtrainer_fit_kwargs\n\u001B[0;32m    136\u001B[0m )\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave_trainer(trainer\u001B[38;5;241m=\u001B[39mtrainer)\n\u001B[0;32m    138\u001B[0m time_end \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\tabular\\trainer\\auto_trainer.py:85\u001B[0m, in \u001B[0;36mAutoTrainer.fit\u001B[1;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, holdout_frac, num_stack_levels, core_kwargs, time_limit, infer_limit, infer_limit_batch_size, use_bag_holdout, groups, **kwargs)\u001B[0m\n\u001B[0;32m     76\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m use_bag_holdout:\n\u001B[0;32m     77\u001B[0m         \u001B[38;5;66;03m# TODO: User could be intending to blend instead. Add support for blend stacking.\u001B[39;00m\n\u001B[0;32m     78\u001B[0m         \u001B[38;5;66;03m#  This error message is necessary because when calculating out-of-fold predictions for user, we want to return them in the form given in train_data,\u001B[39;00m\n\u001B[0;32m     79\u001B[0m         \u001B[38;5;66;03m#  but if we merge train and val here, it becomes very confusing from a users perspective, especially because we reset index, making it impossible to match\u001B[39;00m\n\u001B[0;32m     80\u001B[0m         \u001B[38;5;66;03m#  the original train_data to the out-of-fold predictions from `predictor.get_oof_pred_proba()`.\u001B[39;00m\n\u001B[0;32m     81\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mX_val, y_val is not None, but bagged mode was specified. If calling from `TabularPredictor.fit()`, `tuning_data` must be None.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     82\u001B[0m                              \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBagged mode does not use tuning data / validation data. Instead, all data (`train_data` and `tuning_data`) should be combined and specified as `train_data`.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     83\u001B[0m                              \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBagging/Stacking with a held-out validation set (blend stacking) is not yet supported.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 85\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train_multi_and_ensemble\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     86\u001B[0m \u001B[43m                               \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     87\u001B[0m \u001B[43m                               \u001B[49m\u001B[43mX_val\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     88\u001B[0m \u001B[43m                               \u001B[49m\u001B[43my_val\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     89\u001B[0m \u001B[43m                               \u001B[49m\u001B[43mX_unlabeled\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_unlabeled\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     90\u001B[0m \u001B[43m                               \u001B[49m\u001B[43mhyperparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhyperparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     91\u001B[0m \u001B[43m                               \u001B[49m\u001B[43mnum_stack_levels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_stack_levels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     92\u001B[0m \u001B[43m                               \u001B[49m\u001B[43mtime_limit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtime_limit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     93\u001B[0m \u001B[43m                               \u001B[49m\u001B[43mcore_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcore_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     94\u001B[0m \u001B[43m                               \u001B[49m\u001B[43minfer_limit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_limit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     95\u001B[0m \u001B[43m                               \u001B[49m\u001B[43minfer_limit_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_limit_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     96\u001B[0m \u001B[43m                               \u001B[49m\u001B[43mgroups\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1529\u001B[0m, in \u001B[0;36mAbstractTrainer._train_multi_and_ensemble\u001B[1;34m(self, X, y, X_val, y_val, hyperparameters, X_unlabeled, num_stack_levels, time_limit, groups, **kwargs)\u001B[0m\n\u001B[0;32m   1527\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_rows_train \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(X_val)\n\u001B[0;32m   1528\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_cols_train \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mlist\u001B[39m(X\u001B[38;5;241m.\u001B[39mcolumns))\n\u001B[1;32m-> 1529\u001B[0m model_names_fit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_multi_levels(X, y, hyperparameters\u001B[38;5;241m=\u001B[39mhyperparameters, X_val\u001B[38;5;241m=\u001B[39mX_val, y_val\u001B[38;5;241m=\u001B[39my_val,\n\u001B[0;32m   1530\u001B[0m                                           X_unlabeled\u001B[38;5;241m=\u001B[39mX_unlabeled, level_start\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, level_end\u001B[38;5;241m=\u001B[39mnum_stack_levels\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m, time_limit\u001B[38;5;241m=\u001B[39mtime_limit, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_model_names()) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1532\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAutoGluon did not successfully train any models\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:287\u001B[0m, in \u001B[0;36mAbstractTrainer.train_multi_levels\u001B[1;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, base_model_names, core_kwargs, aux_kwargs, level_start, level_end, time_limit, name_suffix, relative_stack, level_time_modifier, infer_limit, infer_limit_batch_size)\u001B[0m\n\u001B[0;32m    285\u001B[0m         core_kwargs_level[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime_limit\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m core_kwargs_level\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime_limit\u001B[39m\u001B[38;5;124m'\u001B[39m, time_limit_core)\n\u001B[0;32m    286\u001B[0m         aux_kwargs_level[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime_limit\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m aux_kwargs_level\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime_limit\u001B[39m\u001B[38;5;124m'\u001B[39m, time_limit_aux)\n\u001B[1;32m--> 287\u001B[0m     base_model_names, aux_models \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack_new_level\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    288\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_val\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_unlabeled\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_unlabeled\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    289\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhyperparameters\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbase_model_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbase_model_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    290\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcore_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcore_kwargs_level\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maux_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maux_kwargs_level\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname_suffix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname_suffix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    291\u001B[0m \u001B[43m        \u001B[49m\u001B[43minfer_limit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_limit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minfer_limit_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_limit_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    292\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    293\u001B[0m     model_names_fit \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m base_model_names \u001B[38;5;241m+\u001B[39m aux_models\n\u001B[0;32m    294\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time_limit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:397\u001B[0m, in \u001B[0;36mAbstractTrainer.stack_new_level\u001B[1;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, core_kwargs, aux_kwargs, name_suffix, infer_limit, infer_limit_batch_size)\u001B[0m\n\u001B[0;32m    395\u001B[0m     core_kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname_suffix\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m core_kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname_suffix\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m+\u001B[39m name_suffix\n\u001B[0;32m    396\u001B[0m     aux_kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname_suffix\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m aux_kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname_suffix\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m+\u001B[39m name_suffix\n\u001B[1;32m--> 397\u001B[0m core_models \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstack_new_level_core(X\u001B[38;5;241m=\u001B[39mX, y\u001B[38;5;241m=\u001B[39my, X_val\u001B[38;5;241m=\u001B[39mX_val, y_val\u001B[38;5;241m=\u001B[39my_val, X_unlabeled\u001B[38;5;241m=\u001B[39mX_unlabeled, models\u001B[38;5;241m=\u001B[39mmodels,\n\u001B[0;32m    398\u001B[0m                                         level\u001B[38;5;241m=\u001B[39mlevel, infer_limit\u001B[38;5;241m=\u001B[39minfer_limit, infer_limit_batch_size\u001B[38;5;241m=\u001B[39minfer_limit_batch_size, base_model_names\u001B[38;5;241m=\u001B[39mbase_model_names, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcore_kwargs)\n\u001B[0;32m    400\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m X_val \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    401\u001B[0m     aux_models \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstack_new_level_aux(X\u001B[38;5;241m=\u001B[39mX, y\u001B[38;5;241m=\u001B[39my, base_model_names\u001B[38;5;241m=\u001B[39mcore_models, level\u001B[38;5;241m=\u001B[39mlevel\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m    402\u001B[0m                                           infer_limit\u001B[38;5;241m=\u001B[39minfer_limit, infer_limit_batch_size\u001B[38;5;241m=\u001B[39minfer_limit_batch_size, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39maux_kwargs)\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:487\u001B[0m, in \u001B[0;36mAbstractTrainer.stack_new_level_core\u001B[1;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, stack_name, ag_args, ag_args_fit, ag_args_ensemble, excluded_model_types, ensemble_type, name_suffix, get_models_func, refit_full, infer_limit, infer_limit_batch_size, **kwargs)\u001B[0m\n\u001B[0;32m    484\u001B[0m fit_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(num_classes\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_classes)\n\u001B[0;32m    486\u001B[0m \u001B[38;5;66;03m# FIXME: TODO: v0.1 X_unlabeled isn't cached so it won't be available during refit_full or fit_extra.\u001B[39;00m\n\u001B[1;32m--> 487\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_multi(X\u001B[38;5;241m=\u001B[39mX_init, y\u001B[38;5;241m=\u001B[39my, X_val\u001B[38;5;241m=\u001B[39mX_val, y_val\u001B[38;5;241m=\u001B[39my_val, X_unlabeled\u001B[38;5;241m=\u001B[39mX_unlabeled,\n\u001B[0;32m    488\u001B[0m                          models\u001B[38;5;241m=\u001B[39mmodels, level\u001B[38;5;241m=\u001B[39mlevel, stack_name\u001B[38;5;241m=\u001B[39mstack_name, fit_kwargs\u001B[38;5;241m=\u001B[39mfit_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1501\u001B[0m, in \u001B[0;36mAbstractTrainer._train_multi\u001B[1;34m(self, X, y, models, hyperparameter_tune_kwargs, feature_prune_kwargs, k_fold, n_repeats, n_repeat_start, time_limit, **kwargs)\u001B[0m\n\u001B[0;32m   1499\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_repeat_start \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1500\u001B[0m     time_start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m-> 1501\u001B[0m     model_names_trained \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_multi_initial(X\u001B[38;5;241m=\u001B[39mX, y\u001B[38;5;241m=\u001B[39my, models\u001B[38;5;241m=\u001B[39mmodels, k_fold\u001B[38;5;241m=\u001B[39mk_fold, n_repeats\u001B[38;5;241m=\u001B[39mn_repeats_initial, hyperparameter_tune_kwargs\u001B[38;5;241m=\u001B[39mhyperparameter_tune_kwargs,\n\u001B[0;32m   1502\u001B[0m                                                     feature_prune_kwargs\u001B[38;5;241m=\u001B[39mfeature_prune_kwargs, time_limit\u001B[38;5;241m=\u001B[39mtime_limit, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1503\u001B[0m     n_repeat_start \u001B[38;5;241m=\u001B[39m n_repeats_initial\n\u001B[0;32m   1504\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m time_limit \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1387\u001B[0m, in \u001B[0;36mAbstractTrainer._train_multi_initial\u001B[1;34m(self, X, y, models, k_fold, n_repeats, hyperparameter_tune_kwargs, time_limit, feature_prune_kwargs, **kwargs)\u001B[0m\n\u001B[0;32m   1385\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m bagged:\n\u001B[0;32m   1386\u001B[0m     time_ratio \u001B[38;5;241m=\u001B[39m hpo_time_ratio \u001B[38;5;28;01mif\u001B[39;00m hpo_enabled \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 1387\u001B[0m     models \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_multi_fold(models\u001B[38;5;241m=\u001B[39mmodels, hyperparameter_tune_kwargs\u001B[38;5;241m=\u001B[39mhyperparameter_tune_kwargs,\n\u001B[0;32m   1388\u001B[0m                                     time_limit\u001B[38;5;241m=\u001B[39mtime_limit, time_split\u001B[38;5;241m=\u001B[39mtime_split, time_ratio\u001B[38;5;241m=\u001B[39mtime_ratio, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_args)\n\u001B[0;32m   1389\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1390\u001B[0m     bagged_time_start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1472\u001B[0m, in \u001B[0;36mAbstractTrainer._train_multi_fold\u001B[1;34m(self, X, y, models, time_limit, time_split, time_ratio, hyperparameter_tune_kwargs, **kwargs)\u001B[0m\n\u001B[0;32m   1470\u001B[0m         time_start_model \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m   1471\u001B[0m         time_left \u001B[38;5;241m=\u001B[39m time_limit \u001B[38;5;241m-\u001B[39m (time_start_model \u001B[38;5;241m-\u001B[39m time_start)\n\u001B[1;32m-> 1472\u001B[0m model_name_trained_lst \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_single_full(X, y, model, time_limit\u001B[38;5;241m=\u001B[39mtime_left, hyperparameter_tune_kwargs\u001B[38;5;241m=\u001B[39mhyperparameter_tune_kwargs_model, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1474\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlow_memory:\n\u001B[0;32m   1475\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m model\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1296\u001B[0m, in \u001B[0;36mAbstractTrainer._train_single_full\u001B[1;34m(self, X, y, model, X_unlabeled, X_val, y_val, X_pseudo, y_pseudo, feature_prune, hyperparameter_tune_kwargs, stack_name, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, level, time_limit, fit_kwargs, **kwargs)\u001B[0m\n\u001B[0;32m   1294\u001B[0m         bagged_model_fit_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_bagged_model_fit_kwargs(k_fold\u001B[38;5;241m=\u001B[39mk_fold, k_fold_start\u001B[38;5;241m=\u001B[39mk_fold_start, k_fold_end\u001B[38;5;241m=\u001B[39mk_fold_end, n_repeats\u001B[38;5;241m=\u001B[39mn_repeats, n_repeat_start\u001B[38;5;241m=\u001B[39mn_repeat_start)\n\u001B[0;32m   1295\u001B[0m         model_fit_kwargs\u001B[38;5;241m.\u001B[39mupdate(bagged_model_fit_kwargs)\n\u001B[1;32m-> 1296\u001B[0m     model_names_trained \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_and_save(X, y, model, X_val, y_val, X_unlabeled\u001B[38;5;241m=\u001B[39mX_unlabeled, stack_name\u001B[38;5;241m=\u001B[39mstack_name, level\u001B[38;5;241m=\u001B[39mlevel, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_fit_kwargs)\n\u001B[0;32m   1297\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave()\n\u001B[0;32m   1298\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model_names_trained\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1074\u001B[0m, in \u001B[0;36mAbstractTrainer._train_and_save\u001B[1;34m(self, X, y, model, X_val, y_val, stack_name, level, **model_fit_kwargs)\u001B[0m\n\u001B[0;32m   1072\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_single(X_w_pseudo, y_w_pseudo, model, X_val, y_val, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_fit_kwargs)\n\u001B[0;32m   1073\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1074\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_single(X, y, model, X_val, y_val, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_fit_kwargs)\n\u001B[0;32m   1076\u001B[0m fit_end_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m   1077\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight_evaluation:\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1032\u001B[0m, in \u001B[0;36mAbstractTrainer._train_single\u001B[1;34m(self, X, y, model, X_val, y_val, **model_fit_kwargs)\u001B[0m\n\u001B[0;32m   1027\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_train_single\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, model: AbstractModel, X_val\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, y_val\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_fit_kwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m AbstractModel:\n\u001B[0;32m   1028\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1029\u001B[0m \u001B[38;5;124;03m    Trains model but does not add the trained model to this Trainer.\u001B[39;00m\n\u001B[0;32m   1030\u001B[0m \u001B[38;5;124;03m    Returns trained model object.\u001B[39;00m\n\u001B[0;32m   1031\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1032\u001B[0m     model \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mfit(X\u001B[38;5;241m=\u001B[39mX, y\u001B[38;5;241m=\u001B[39my, X_val\u001B[38;5;241m=\u001B[39mX_val, y_val\u001B[38;5;241m=\u001B[39my_val, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_fit_kwargs)\n\u001B[0;32m   1033\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py:577\u001B[0m, in \u001B[0;36mAbstractModel.fit\u001B[1;34m(self, **kwargs)\u001B[0m\n\u001B[0;32m    575\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_register_fit_metadata(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    576\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_fit_memory_usage(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m--> 577\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    578\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m out \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    579\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py:196\u001B[0m, in \u001B[0;36mTabularNeuralNetTorchModel._fit\u001B[1;34m(self, X, y, X_val, y_val, time_limit, sample_weight, num_cpus, num_gpus, reporter, verbosity, **kwargs)\u001B[0m\n\u001B[0;32m    193\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m TimeLimitExceeded\n\u001B[0;32m    195\u001B[0m \u001B[38;5;66;03m# train network\u001B[39;00m\n\u001B[1;32m--> 196\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_net(train_dataset\u001B[38;5;241m=\u001B[39mtrain_dataset,\n\u001B[0;32m    197\u001B[0m                 loss_kwargs\u001B[38;5;241m=\u001B[39mloss_kwargs,\n\u001B[0;32m    198\u001B[0m                 batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[0;32m    199\u001B[0m                 val_dataset\u001B[38;5;241m=\u001B[39mval_dataset,\n\u001B[0;32m    200\u001B[0m                 time_limit\u001B[38;5;241m=\u001B[39mtime_limit,\n\u001B[0;32m    201\u001B[0m                 reporter\u001B[38;5;241m=\u001B[39mreporter,\n\u001B[0;32m    202\u001B[0m                 verbosity\u001B[38;5;241m=\u001B[39mverbosity,\n\u001B[0;32m    203\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_kwargs)\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py:300\u001B[0m, in \u001B[0;36mTabularNeuralNetTorchModel._train_net\u001B[1;34m(self, train_dataset, loss_kwargs, batch_size, num_epochs, epochs_wo_improve, val_dataset, time_limit, reporter, verbosity)\u001B[0m\n\u001B[0;32m    298\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m    299\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m--> 300\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    301\u001B[0m total_updates \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    303\u001B[0m \u001B[38;5;66;03m# time limit\u001B[39;00m\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\torch\\optim\\optimizer.py:88\u001B[0m, in \u001B[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     86\u001B[0m profile_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimizer.step#\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.step\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m     87\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(profile_name):\n\u001B[1;32m---> 88\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\torch\\autograd\\grad_mode.py:28\u001B[0m, in \u001B[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m():\n\u001B[1;32m---> 28\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\torch\\optim\\adam.py:133\u001B[0m, in \u001B[0;36mAdam.step\u001B[1;34m(self, closure)\u001B[0m\n\u001B[0;32m    130\u001B[0m             \u001B[38;5;66;03m# record the step after step update\u001B[39;00m\n\u001B[0;32m    131\u001B[0m             state_steps\u001B[38;5;241m.\u001B[39mappend(state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstep\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m--> 133\u001B[0m     \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    134\u001B[0m \u001B[43m           \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    135\u001B[0m \u001B[43m           \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    136\u001B[0m \u001B[43m           \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    137\u001B[0m \u001B[43m           \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    138\u001B[0m \u001B[43m           \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    139\u001B[0m \u001B[43m           \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mamsgrad\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    140\u001B[0m \u001B[43m           \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    141\u001B[0m \u001B[43m           \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    142\u001B[0m \u001B[43m           \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    143\u001B[0m \u001B[43m           \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    144\u001B[0m \u001B[43m           \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    145\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\torch\\optim\\_functional.py:87\u001B[0m, in \u001B[0;36madam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001B[0m\n\u001B[0;32m     85\u001B[0m \u001B[38;5;66;03m# Decay the first and second moment running average coefficient\u001B[39;00m\n\u001B[0;32m     86\u001B[0m exp_avg\u001B[38;5;241m.\u001B[39mmul_(beta1)\u001B[38;5;241m.\u001B[39madd_(grad, alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta1)\n\u001B[1;32m---> 87\u001B[0m \u001B[43mexp_avg_sq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmul_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maddcmul_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconj\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     88\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m amsgrad:\n\u001B[0;32m     89\u001B[0m     \u001B[38;5;66;03m# Maintains the maximum of all 2nd moment running avg. till now\u001B[39;00m\n\u001B[0;32m     90\u001B[0m     torch\u001B[38;5;241m.\u001B[39mmaximum(max_exp_avg_sqs[i], exp_avg_sq, out\u001B[38;5;241m=\u001B[39mmax_exp_avg_sqs[i])\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "train_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv')\n",
    "test_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv')\n",
    "savepath='test_gpu'\n",
    "predictor = TabularPredictor(label='class',path=savepath).fit(train_data=train_data,ag_args_fit={'num_gpus': 1})\n",
    "predictions = predictor.predict(test_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}