{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_10860\\1433843627.py:9: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(data_path) # path to colab notebook #replace this with the path to your dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": "   W1mag-W2mag  W2mag-W3mag  Jmag-Hmag  Jmag-W1mag  Jmag-W2mag  Jmag-Kmag  \\\n0     0.290000        0.824   0.846000    2.123000    2.413000   1.515000   \n1     0.297999        1.277   0.860999    1.724000    2.021999   1.433000   \n2     0.326000        1.328   1.146000    2.237000    2.563000   1.566000   \n3     0.492001        1.438   0.837000    2.266999    2.759000   1.407000   \n4     0.316000        0.303   0.908000    2.028999    2.344999   1.481999   \n\n   subclass  \n0         0  \n1         0  \n2         0  \n3         0  \n4         0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>W1mag-W2mag</th>\n      <th>W2mag-W3mag</th>\n      <th>Jmag-Hmag</th>\n      <th>Jmag-W1mag</th>\n      <th>Jmag-W2mag</th>\n      <th>Jmag-Kmag</th>\n      <th>subclass</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.290000</td>\n      <td>0.824</td>\n      <td>0.846000</td>\n      <td>2.123000</td>\n      <td>2.413000</td>\n      <td>1.515000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.297999</td>\n      <td>1.277</td>\n      <td>0.860999</td>\n      <td>1.724000</td>\n      <td>2.021999</td>\n      <td>1.433000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.326000</td>\n      <td>1.328</td>\n      <td>1.146000</td>\n      <td>2.237000</td>\n      <td>2.563000</td>\n      <td>1.566000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.492001</td>\n      <td>1.438</td>\n      <td>0.837000</td>\n      <td>2.266999</td>\n      <td>2.759000</td>\n      <td>1.407000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.316000</td>\n      <td>0.303</td>\n      <td>0.908000</td>\n      <td>2.028999</td>\n      <td>2.344999</td>\n      <td>1.481999</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_path= \"result_20220309_color.csv\"\n",
    "df=pd.read_csv(data_path) # path to colab notebook #replace this with the path to your dataset\n",
    "df_use=df.iloc[:,4:11]\n",
    "df_use.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"./dich_BD/\"\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (627047 samples, 57.04 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"./dich_BD/\\\"\n",
      "AutoGluon Version:  0.4.0\n",
      "Python Version:     3.9.11\n",
      "Operating System:   Windows\n",
      "Train Data Rows:    627047\n",
      "Train Data Columns: 6\n",
      "Label Column: subclass\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3427.58 MB\n",
      "\tTrain Data (Original)  Memory Usage: 30.1 MB (0.9% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 6 | ['W1mag-W2mag', 'W2mag-W3mag', 'Jmag-Hmag', 'Jmag-W1mag', 'Jmag-W2mag', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 6 | ['W1mag-W2mag', 'W2mag-W3mag', 'Jmag-Hmag', 'Jmag-W1mag', 'Jmag-W2mag', ...]\n",
      "\t0.9s = Fit runtime\n",
      "\t6 features in original data used to generate 6 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 30.1 MB (0.9% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.09s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.01, Train Rows: 620776, Val Rows: 6271\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.9925\t = Validation score   (accuracy)\n",
      "\t2.71s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.9925\t = Validation score   (accuracy)\n",
      "\t2.73s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t0.993\t = Validation score   (accuracy)\n",
      "\t2.59s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.993\t = Validation score   (accuracy)\n",
      "\t0.86s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.993\t = Validation score   (accuracy)\n",
      "\t106.06s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.993\t = Validation score   (accuracy)\n",
      "\t99.61s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.993\t = Validation score   (accuracy)\n",
      "\t2.11s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 276 due to low memory. Expected memory usage reduced from 16.28% -> 15.0% of available memory...\n",
      "\t0.993\t = Validation score   (accuracy)\n",
      "\t40.74s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 271 due to low memory. Expected memory usage reduced from 16.57% -> 15.0% of available memory...\n",
      "\t0.993\t = Validation score   (accuracy)\n",
      "\t39.62s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 0: early stopping\n",
      "\t0.993\t = Validation score   (accuracy)\n",
      "\t256.44s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "D:\\conda\\envs\\automl\\lib\\site-packages\\xgboost\\compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "\t0.993\t = Validation score   (accuracy)\n",
      "\t2.64s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [3]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m#test_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv')\u001B[39;00m\n\u001B[0;32m      4\u001B[0m save_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./dich_BD/\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m----> 5\u001B[0m predictor \u001B[38;5;241m=\u001B[39m \u001B[43mTabularPredictor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msubclass\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msave_path\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdf_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\core\\utils\\decorators.py:30\u001B[0m, in \u001B[0;36munpack.<locals>._unpack_inner.<locals>._call\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(f)\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_call\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     29\u001B[0m     gargs, gkwargs \u001B[38;5;241m=\u001B[39m g(\u001B[38;5;241m*\u001B[39mother_args, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m---> 30\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39mgargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mgkwargs)\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:813\u001B[0m, in \u001B[0;36mTabularPredictor.fit\u001B[1;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, **kwargs)\u001B[0m\n\u001B[0;32m    805\u001B[0m core_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    806\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mag_args\u001B[39m\u001B[38;5;124m'\u001B[39m: ag_args,\n\u001B[0;32m    807\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mag_args_ensemble\u001B[39m\u001B[38;5;124m'\u001B[39m: ag_args_ensemble,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    810\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfeature_prune_kwargs\u001B[39m\u001B[38;5;124m'\u001B[39m: kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfeature_prune_kwargs\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    811\u001B[0m }\n\u001B[0;32m    812\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave(silent\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)  \u001B[38;5;66;03m# Save predictor to disk to enable prediction and training after interrupt\u001B[39;00m\n\u001B[1;32m--> 813\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_learner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_val\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtuning_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_unlabeled\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43munlabeled_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    814\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mholdout_frac\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mholdout_frac\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_bag_folds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_bag_folds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_bag_sets\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_bag_sets\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    815\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mnum_stack_levels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_stack_levels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    816\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mhyperparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhyperparameters\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcore_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcore_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    817\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mtime_limit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtime_limit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minfer_limit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_limit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minfer_limit_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_limit_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    818\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mverbosity\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbosity\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_bag_holdout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_bag_holdout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    819\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_post_fit_vars()\n\u001B[0;32m    821\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_post_fit(\n\u001B[0;32m    822\u001B[0m     keep_only_best\u001B[38;5;241m=\u001B[39mkwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mkeep_only_best\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    823\u001B[0m     refit_full\u001B[38;5;241m=\u001B[39mkwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrefit_full\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    827\u001B[0m     infer_limit\u001B[38;5;241m=\u001B[39minfer_limit,\n\u001B[0;32m    828\u001B[0m )\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\tabular\\learner\\abstract_learner.py:150\u001B[0m, in \u001B[0;36mAbstractLearner.fit\u001B[1;34m(self, X, X_val, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLearner is already fit.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_fit_input(X\u001B[38;5;241m=\u001B[39mX, X_val\u001B[38;5;241m=\u001B[39mX_val, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m--> 150\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(X\u001B[38;5;241m=\u001B[39mX, X_val\u001B[38;5;241m=\u001B[39mX_val, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\tabular\\learner\\default_learner.py:124\u001B[0m, in \u001B[0;36mDefaultLearner._fit\u001B[1;34m(self, X, X_val, X_unlabeled, holdout_frac, num_bag_folds, num_bag_sets, time_limit, infer_limit, infer_limit_batch_size, verbosity, **trainer_fit_kwargs)\u001B[0m\n\u001B[0;32m    121\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39meval_metric \u001B[38;5;241m=\u001B[39m trainer\u001B[38;5;241m.\u001B[39meval_metric\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave()\n\u001B[1;32m--> 124\u001B[0m trainer\u001B[38;5;241m.\u001B[39mfit(\n\u001B[0;32m    125\u001B[0m     X\u001B[38;5;241m=\u001B[39mX,\n\u001B[0;32m    126\u001B[0m     y\u001B[38;5;241m=\u001B[39my,\n\u001B[0;32m    127\u001B[0m     X_val\u001B[38;5;241m=\u001B[39mX_val,\n\u001B[0;32m    128\u001B[0m     y_val\u001B[38;5;241m=\u001B[39my_val,\n\u001B[0;32m    129\u001B[0m     X_unlabeled\u001B[38;5;241m=\u001B[39mX_unlabeled,\n\u001B[0;32m    130\u001B[0m     holdout_frac\u001B[38;5;241m=\u001B[39mholdout_frac,\n\u001B[0;32m    131\u001B[0m     time_limit\u001B[38;5;241m=\u001B[39mtime_limit_trainer,\n\u001B[0;32m    132\u001B[0m     infer_limit\u001B[38;5;241m=\u001B[39minfer_limit,\n\u001B[0;32m    133\u001B[0m     infer_limit_batch_size\u001B[38;5;241m=\u001B[39minfer_limit_batch_size,\n\u001B[0;32m    134\u001B[0m     groups\u001B[38;5;241m=\u001B[39mgroups,\n\u001B[0;32m    135\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtrainer_fit_kwargs\n\u001B[0;32m    136\u001B[0m )\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave_trainer(trainer\u001B[38;5;241m=\u001B[39mtrainer)\n\u001B[0;32m    138\u001B[0m time_end \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\tabular\\trainer\\auto_trainer.py:85\u001B[0m, in \u001B[0;36mAutoTrainer.fit\u001B[1;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, holdout_frac, num_stack_levels, core_kwargs, time_limit, infer_limit, infer_limit_batch_size, use_bag_holdout, groups, **kwargs)\u001B[0m\n\u001B[0;32m     76\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m use_bag_holdout:\n\u001B[0;32m     77\u001B[0m         \u001B[38;5;66;03m# TODO: User could be intending to blend instead. Add support for blend stacking.\u001B[39;00m\n\u001B[0;32m     78\u001B[0m         \u001B[38;5;66;03m#  This error message is necessary because when calculating out-of-fold predictions for user, we want to return them in the form given in train_data,\u001B[39;00m\n\u001B[0;32m     79\u001B[0m         \u001B[38;5;66;03m#  but if we merge train and val here, it becomes very confusing from a users perspective, especially because we reset index, making it impossible to match\u001B[39;00m\n\u001B[0;32m     80\u001B[0m         \u001B[38;5;66;03m#  the original train_data to the out-of-fold predictions from `predictor.get_oof_pred_proba()`.\u001B[39;00m\n\u001B[0;32m     81\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mX_val, y_val is not None, but bagged mode was specified. If calling from `TabularPredictor.fit()`, `tuning_data` must be None.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     82\u001B[0m                              \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBagged mode does not use tuning data / validation data. Instead, all data (`train_data` and `tuning_data`) should be combined and specified as `train_data`.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     83\u001B[0m                              \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBagging/Stacking with a held-out validation set (blend stacking) is not yet supported.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 85\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train_multi_and_ensemble\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     86\u001B[0m \u001B[43m                               \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     87\u001B[0m \u001B[43m                               \u001B[49m\u001B[43mX_val\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     88\u001B[0m \u001B[43m                               \u001B[49m\u001B[43my_val\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     89\u001B[0m \u001B[43m                               \u001B[49m\u001B[43mX_unlabeled\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_unlabeled\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     90\u001B[0m \u001B[43m                               \u001B[49m\u001B[43mhyperparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhyperparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     91\u001B[0m \u001B[43m                               \u001B[49m\u001B[43mnum_stack_levels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_stack_levels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     92\u001B[0m \u001B[43m                               \u001B[49m\u001B[43mtime_limit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtime_limit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     93\u001B[0m \u001B[43m                               \u001B[49m\u001B[43mcore_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcore_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     94\u001B[0m \u001B[43m                               \u001B[49m\u001B[43minfer_limit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_limit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     95\u001B[0m \u001B[43m                               \u001B[49m\u001B[43minfer_limit_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_limit_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     96\u001B[0m \u001B[43m                               \u001B[49m\u001B[43mgroups\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1529\u001B[0m, in \u001B[0;36mAbstractTrainer._train_multi_and_ensemble\u001B[1;34m(self, X, y, X_val, y_val, hyperparameters, X_unlabeled, num_stack_levels, time_limit, groups, **kwargs)\u001B[0m\n\u001B[0;32m   1527\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_rows_train \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(X_val)\n\u001B[0;32m   1528\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_cols_train \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mlist\u001B[39m(X\u001B[38;5;241m.\u001B[39mcolumns))\n\u001B[1;32m-> 1529\u001B[0m model_names_fit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_multi_levels(X, y, hyperparameters\u001B[38;5;241m=\u001B[39mhyperparameters, X_val\u001B[38;5;241m=\u001B[39mX_val, y_val\u001B[38;5;241m=\u001B[39my_val,\n\u001B[0;32m   1530\u001B[0m                                           X_unlabeled\u001B[38;5;241m=\u001B[39mX_unlabeled, level_start\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, level_end\u001B[38;5;241m=\u001B[39mnum_stack_levels\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m, time_limit\u001B[38;5;241m=\u001B[39mtime_limit, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_model_names()) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1532\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAutoGluon did not successfully train any models\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:287\u001B[0m, in \u001B[0;36mAbstractTrainer.train_multi_levels\u001B[1;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, base_model_names, core_kwargs, aux_kwargs, level_start, level_end, time_limit, name_suffix, relative_stack, level_time_modifier, infer_limit, infer_limit_batch_size)\u001B[0m\n\u001B[0;32m    285\u001B[0m         core_kwargs_level[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime_limit\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m core_kwargs_level\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime_limit\u001B[39m\u001B[38;5;124m'\u001B[39m, time_limit_core)\n\u001B[0;32m    286\u001B[0m         aux_kwargs_level[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime_limit\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m aux_kwargs_level\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime_limit\u001B[39m\u001B[38;5;124m'\u001B[39m, time_limit_aux)\n\u001B[1;32m--> 287\u001B[0m     base_model_names, aux_models \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack_new_level\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    288\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_val\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_unlabeled\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_unlabeled\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    289\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhyperparameters\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbase_model_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbase_model_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    290\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcore_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcore_kwargs_level\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maux_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maux_kwargs_level\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname_suffix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname_suffix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    291\u001B[0m \u001B[43m        \u001B[49m\u001B[43minfer_limit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_limit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minfer_limit_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_limit_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    292\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    293\u001B[0m     model_names_fit \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m base_model_names \u001B[38;5;241m+\u001B[39m aux_models\n\u001B[0;32m    294\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time_limit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:397\u001B[0m, in \u001B[0;36mAbstractTrainer.stack_new_level\u001B[1;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, core_kwargs, aux_kwargs, name_suffix, infer_limit, infer_limit_batch_size)\u001B[0m\n\u001B[0;32m    395\u001B[0m     core_kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname_suffix\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m core_kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname_suffix\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m+\u001B[39m name_suffix\n\u001B[0;32m    396\u001B[0m     aux_kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname_suffix\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m aux_kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname_suffix\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m+\u001B[39m name_suffix\n\u001B[1;32m--> 397\u001B[0m core_models \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstack_new_level_core(X\u001B[38;5;241m=\u001B[39mX, y\u001B[38;5;241m=\u001B[39my, X_val\u001B[38;5;241m=\u001B[39mX_val, y_val\u001B[38;5;241m=\u001B[39my_val, X_unlabeled\u001B[38;5;241m=\u001B[39mX_unlabeled, models\u001B[38;5;241m=\u001B[39mmodels,\n\u001B[0;32m    398\u001B[0m                                         level\u001B[38;5;241m=\u001B[39mlevel, infer_limit\u001B[38;5;241m=\u001B[39minfer_limit, infer_limit_batch_size\u001B[38;5;241m=\u001B[39minfer_limit_batch_size, base_model_names\u001B[38;5;241m=\u001B[39mbase_model_names, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcore_kwargs)\n\u001B[0;32m    400\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m X_val \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    401\u001B[0m     aux_models \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstack_new_level_aux(X\u001B[38;5;241m=\u001B[39mX, y\u001B[38;5;241m=\u001B[39my, base_model_names\u001B[38;5;241m=\u001B[39mcore_models, level\u001B[38;5;241m=\u001B[39mlevel\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m    402\u001B[0m                                           infer_limit\u001B[38;5;241m=\u001B[39minfer_limit, infer_limit_batch_size\u001B[38;5;241m=\u001B[39minfer_limit_batch_size, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39maux_kwargs)\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:487\u001B[0m, in \u001B[0;36mAbstractTrainer.stack_new_level_core\u001B[1;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, stack_name, ag_args, ag_args_fit, ag_args_ensemble, excluded_model_types, ensemble_type, name_suffix, get_models_func, refit_full, infer_limit, infer_limit_batch_size, **kwargs)\u001B[0m\n\u001B[0;32m    484\u001B[0m fit_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(num_classes\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_classes)\n\u001B[0;32m    486\u001B[0m \u001B[38;5;66;03m# FIXME: TODO: v0.1 X_unlabeled isn't cached so it won't be available during refit_full or fit_extra.\u001B[39;00m\n\u001B[1;32m--> 487\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_multi(X\u001B[38;5;241m=\u001B[39mX_init, y\u001B[38;5;241m=\u001B[39my, X_val\u001B[38;5;241m=\u001B[39mX_val, y_val\u001B[38;5;241m=\u001B[39my_val, X_unlabeled\u001B[38;5;241m=\u001B[39mX_unlabeled,\n\u001B[0;32m    488\u001B[0m                          models\u001B[38;5;241m=\u001B[39mmodels, level\u001B[38;5;241m=\u001B[39mlevel, stack_name\u001B[38;5;241m=\u001B[39mstack_name, fit_kwargs\u001B[38;5;241m=\u001B[39mfit_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1501\u001B[0m, in \u001B[0;36mAbstractTrainer._train_multi\u001B[1;34m(self, X, y, models, hyperparameter_tune_kwargs, feature_prune_kwargs, k_fold, n_repeats, n_repeat_start, time_limit, **kwargs)\u001B[0m\n\u001B[0;32m   1499\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_repeat_start \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1500\u001B[0m     time_start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m-> 1501\u001B[0m     model_names_trained \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_multi_initial(X\u001B[38;5;241m=\u001B[39mX, y\u001B[38;5;241m=\u001B[39my, models\u001B[38;5;241m=\u001B[39mmodels, k_fold\u001B[38;5;241m=\u001B[39mk_fold, n_repeats\u001B[38;5;241m=\u001B[39mn_repeats_initial, hyperparameter_tune_kwargs\u001B[38;5;241m=\u001B[39mhyperparameter_tune_kwargs,\n\u001B[0;32m   1502\u001B[0m                                                     feature_prune_kwargs\u001B[38;5;241m=\u001B[39mfeature_prune_kwargs, time_limit\u001B[38;5;241m=\u001B[39mtime_limit, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1503\u001B[0m     n_repeat_start \u001B[38;5;241m=\u001B[39m n_repeats_initial\n\u001B[0;32m   1504\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m time_limit \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1387\u001B[0m, in \u001B[0;36mAbstractTrainer._train_multi_initial\u001B[1;34m(self, X, y, models, k_fold, n_repeats, hyperparameter_tune_kwargs, time_limit, feature_prune_kwargs, **kwargs)\u001B[0m\n\u001B[0;32m   1385\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m bagged:\n\u001B[0;32m   1386\u001B[0m     time_ratio \u001B[38;5;241m=\u001B[39m hpo_time_ratio \u001B[38;5;28;01mif\u001B[39;00m hpo_enabled \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 1387\u001B[0m     models \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_multi_fold(models\u001B[38;5;241m=\u001B[39mmodels, hyperparameter_tune_kwargs\u001B[38;5;241m=\u001B[39mhyperparameter_tune_kwargs,\n\u001B[0;32m   1388\u001B[0m                                     time_limit\u001B[38;5;241m=\u001B[39mtime_limit, time_split\u001B[38;5;241m=\u001B[39mtime_split, time_ratio\u001B[38;5;241m=\u001B[39mtime_ratio, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_args)\n\u001B[0;32m   1389\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1390\u001B[0m     bagged_time_start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1472\u001B[0m, in \u001B[0;36mAbstractTrainer._train_multi_fold\u001B[1;34m(self, X, y, models, time_limit, time_split, time_ratio, hyperparameter_tune_kwargs, **kwargs)\u001B[0m\n\u001B[0;32m   1470\u001B[0m         time_start_model \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m   1471\u001B[0m         time_left \u001B[38;5;241m=\u001B[39m time_limit \u001B[38;5;241m-\u001B[39m (time_start_model \u001B[38;5;241m-\u001B[39m time_start)\n\u001B[1;32m-> 1472\u001B[0m model_name_trained_lst \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_single_full(X, y, model, time_limit\u001B[38;5;241m=\u001B[39mtime_left, hyperparameter_tune_kwargs\u001B[38;5;241m=\u001B[39mhyperparameter_tune_kwargs_model, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1474\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlow_memory:\n\u001B[0;32m   1475\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m model\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1296\u001B[0m, in \u001B[0;36mAbstractTrainer._train_single_full\u001B[1;34m(self, X, y, model, X_unlabeled, X_val, y_val, X_pseudo, y_pseudo, feature_prune, hyperparameter_tune_kwargs, stack_name, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, level, time_limit, fit_kwargs, **kwargs)\u001B[0m\n\u001B[0;32m   1294\u001B[0m         bagged_model_fit_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_bagged_model_fit_kwargs(k_fold\u001B[38;5;241m=\u001B[39mk_fold, k_fold_start\u001B[38;5;241m=\u001B[39mk_fold_start, k_fold_end\u001B[38;5;241m=\u001B[39mk_fold_end, n_repeats\u001B[38;5;241m=\u001B[39mn_repeats, n_repeat_start\u001B[38;5;241m=\u001B[39mn_repeat_start)\n\u001B[0;32m   1295\u001B[0m         model_fit_kwargs\u001B[38;5;241m.\u001B[39mupdate(bagged_model_fit_kwargs)\n\u001B[1;32m-> 1296\u001B[0m     model_names_trained \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_and_save(X, y, model, X_val, y_val, X_unlabeled\u001B[38;5;241m=\u001B[39mX_unlabeled, stack_name\u001B[38;5;241m=\u001B[39mstack_name, level\u001B[38;5;241m=\u001B[39mlevel, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_fit_kwargs)\n\u001B[0;32m   1297\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave()\n\u001B[0;32m   1298\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model_names_trained\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1074\u001B[0m, in \u001B[0;36mAbstractTrainer._train_and_save\u001B[1;34m(self, X, y, model, X_val, y_val, stack_name, level, **model_fit_kwargs)\u001B[0m\n\u001B[0;32m   1072\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_single(X_w_pseudo, y_w_pseudo, model, X_val, y_val, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_fit_kwargs)\n\u001B[0;32m   1073\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1074\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_single(X, y, model, X_val, y_val, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_fit_kwargs)\n\u001B[0;32m   1076\u001B[0m fit_end_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m   1077\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight_evaluation:\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1032\u001B[0m, in \u001B[0;36mAbstractTrainer._train_single\u001B[1;34m(self, X, y, model, X_val, y_val, **model_fit_kwargs)\u001B[0m\n\u001B[0;32m   1027\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_train_single\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, model: AbstractModel, X_val\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, y_val\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_fit_kwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m AbstractModel:\n\u001B[0;32m   1028\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1029\u001B[0m \u001B[38;5;124;03m    Trains model but does not add the trained model to this Trainer.\u001B[39;00m\n\u001B[0;32m   1030\u001B[0m \u001B[38;5;124;03m    Returns trained model object.\u001B[39;00m\n\u001B[0;32m   1031\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1032\u001B[0m     model \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mfit(X\u001B[38;5;241m=\u001B[39mX, y\u001B[38;5;241m=\u001B[39my, X_val\u001B[38;5;241m=\u001B[39mX_val, y_val\u001B[38;5;241m=\u001B[39my_val, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_fit_kwargs)\n\u001B[0;32m   1033\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py:577\u001B[0m, in \u001B[0;36mAbstractModel.fit\u001B[1;34m(self, **kwargs)\u001B[0m\n\u001B[0;32m    575\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_register_fit_metadata(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    576\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_fit_memory_usage(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m--> 577\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    578\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m out \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    579\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py:196\u001B[0m, in \u001B[0;36mTabularNeuralNetTorchModel._fit\u001B[1;34m(self, X, y, X_val, y_val, time_limit, sample_weight, num_cpus, num_gpus, reporter, verbosity, **kwargs)\u001B[0m\n\u001B[0;32m    193\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m TimeLimitExceeded\n\u001B[0;32m    195\u001B[0m \u001B[38;5;66;03m# train network\u001B[39;00m\n\u001B[1;32m--> 196\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_net(train_dataset\u001B[38;5;241m=\u001B[39mtrain_dataset,\n\u001B[0;32m    197\u001B[0m                 loss_kwargs\u001B[38;5;241m=\u001B[39mloss_kwargs,\n\u001B[0;32m    198\u001B[0m                 batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[0;32m    199\u001B[0m                 val_dataset\u001B[38;5;241m=\u001B[39mval_dataset,\n\u001B[0;32m    200\u001B[0m                 time_limit\u001B[38;5;241m=\u001B[39mtime_limit,\n\u001B[0;32m    201\u001B[0m                 reporter\u001B[38;5;241m=\u001B[39mreporter,\n\u001B[0;32m    202\u001B[0m                 verbosity\u001B[38;5;241m=\u001B[39mverbosity,\n\u001B[0;32m    203\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_kwargs)\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py:299\u001B[0m, in \u001B[0;36mTabularNeuralNetTorchModel._train_net\u001B[1;34m(self, train_dataset, loss_kwargs, batch_size, num_epochs, epochs_wo_improve, val_dataset, time_limit, reporter, verbosity)\u001B[0m\n\u001B[0;32m    297\u001B[0m \u001B[38;5;66;03m# update\u001B[39;00m\n\u001B[0;32m    298\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m--> 299\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    300\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m    301\u001B[0m total_updates \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\torch\\_tensor.py:307\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    298\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    299\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    300\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    301\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    305\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[0;32m    306\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[1;32m--> 307\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\conda\\envs\\automl\\lib\\site-packages\\torch\\autograd\\__init__.py:154\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retain_graph \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    152\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m--> 154\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    155\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    156\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#bd_data = TabularDataset(data_path)\n",
    "df_train,df_test=train_test_split(df_use,test_size=0.15,random_state=1)\n",
    "#test_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv')\n",
    "save_path = './dich_BD/'\n",
    "predictor = TabularPredictor(label='subclass',path=save_path).fit(train_data=df_train)\n",
    "#num_stack_levels=1,num_bag_folds=5\n",
    "#predictions = predictor.predict(test_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}